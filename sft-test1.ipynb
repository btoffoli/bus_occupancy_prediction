{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006f15ac",
   "metadata": {},
   "source": [
    "Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f542d-173e-4575-a23a-4df666e96f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "def list_devices():\n",
    "    # Verifica se CUDA (GPU) está disponível\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA (GPU) está disponível.\")\n",
    "        # Lista todos os dispositivos GPU disponíveis\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    else:\n",
    "        print(\"CUDA (GPU) não está disponível. Usando CPU.\")\n",
    "\n",
    "    # Lista o dispositivo padrão\n",
    "    print(f\"Dispositivo padrão: {torch.cuda.current_device()}\")\n",
    "\n",
    "list_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600e1fc",
   "metadata": {},
   "source": [
    "Iniciando modelo e criando função de tokenização de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mlname = 'unsloth/Qwen2.5-0.5B-bnb-4bit'\n",
    "mlname = 'unsloth/tinyllama-bnb-4bit'\n",
    "# mlname = 'microsoft/phi-2'\n",
    "\n",
    "max_seq_length=512 #2048 #1024\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/tinyllama-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,  # None for auto detection, or torch.float16, or torch.bfloat16\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Prepare model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "print(f\"model.device: {model.device}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_text(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to('cuda:0')\n",
    "    print(f\"inputs: {inputs}\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "    print(f\"outputs: {outputs}\")\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "    return result\n",
    "\n",
    "generate_text(\"<human>: Voce é qual inteligencia artificial?\\n<bot>:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e475b7ba",
   "metadata": {},
   "source": [
    "Iniciando modelo para ser pre treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ffd50ca-1b9a-4999-b327-a347e445dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurando\n",
    "peft_model = FastLanguageModel.get_peft_model(\n",
    "    model=model, r=16, \n",
    "    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias='none',\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=3407,\n",
    "    max_seq_length=max_seq_length,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87816af5",
   "metadata": {},
   "source": [
    "Iniciando o fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521e1341",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PeftModelForCausalLM' object has no attribute '_flag_for_generation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ds = load_dataset(\"laion/OIG\", \"unified_chip2.jsonl\",  split=\"train[:1%]\")\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# ds = load_dataset(\"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\")\u001b[39;00m\n\u001b[32m      4\u001b[39m ds = load_dataset(\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     data_files=\u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     split=\u001b[33m\"\u001b[39m\u001b[33mtrain[:1\u001b[39m\u001b[33m%\u001b[39m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m  \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m  \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m  \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_bf16_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_bf16_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43madamw_8bit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlinear\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3047\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ufes/gpt/bus_occupancy_prediction/.venv/lib/python3.11/site-packages/unsloth/trainer.py:203\u001b[39m, in \u001b[36m_backwards_compatible_trainer.<locals>.new_init\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    201\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m] = config\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ufes/gpt/bus_occupancy_prediction/unsloth_compiled_cache/UnslothSFTTrainer.py:906\u001b[39m, in \u001b[36mUnslothSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func, **kwargs)\u001b[39m\n\u001b[32m    904\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(args, \u001b[33m'\u001b[39m\u001b[33mmax_seq_length\u001b[39m\u001b[33m'\u001b[39m): args.max_seq_length = max_seq_length\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m'\u001b[39m\u001b[33mfor_training\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfor_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtokenizer\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[33m'\u001b[39m\u001b[33mpadding_side\u001b[39m\u001b[33m'\u001b[39m): tokenizer.padding_side = \u001b[33m'\u001b[39m\u001b[33mright\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mprocessing_class\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ufes/gpt/bus_occupancy_prediction/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:2668\u001b[39m, in \u001b[36mFastLlamaModel.for_training\u001b[39m\u001b[34m(model, use_gradient_checkpointing)\u001b[39m\n\u001b[32m   2666\u001b[39m m = model\n\u001b[32m   2667\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2668\u001b[39m     \u001b[43m_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2669\u001b[39m     m = m.model\n\u001b[32m   2670\u001b[39m _for_training(m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ufes/gpt/bus_occupancy_prediction/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:2664\u001b[39m, in \u001b[36mFastLlamaModel.for_training.<locals>._for_training\u001b[39m\u001b[34m(m)\u001b[39m\n\u001b[32m   2662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(m, \u001b[33m\"\u001b[39m\u001b[33m_saved_temp_tokenizer\u001b[39m\u001b[33m\"\u001b[39m): m._saved_temp_tokenizer.padding_side = \u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2663\u001b[39m \u001b[38;5;66;03m# Set a flag for generation!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(m, \u001b[33m\"\u001b[39m\u001b[33m_flag_for_generation\u001b[39m\u001b[33m\"\u001b[39m): \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_flag_for_generation\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ufes/gpt/bus_occupancy_prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2040\u001b[39m, in \u001b[36mModule.__delattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2038\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules[name]\n\u001b[32m   2039\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2040\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__delattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'PeftModelForCausalLM' object has no attribute '_flag_for_generation'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ds = load_dataset(\"laion/OIG\", \"unified_chip2.jsonl\",  split=\"train[:1%]\")\n",
    "# ds = load_dataset(\"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\")\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\",\n",
    "    split=\"train[:1%]\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=peft_model,\n",
    "  train_dataset=ds,\n",
    "  max_seq_length=max_seq_length,\n",
    "  tokenizer=tokenizer,\n",
    "  args=TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    max_steps=60,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    output_dir='./logs',\n",
    "    optim='adamw_8bit',\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='linear',\n",
    "    seed=3047\n",
    "  ),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
